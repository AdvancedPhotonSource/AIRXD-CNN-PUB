{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio as iio\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#AIRXD model import\n",
    "from airxd.mask import MASK\n",
    "from airxd_cnn.cnn import ARIXD_CNN as cmodel\n",
    "from airxd.model import ARIXD\n",
    "from airxd.dataset import Dataset, parse_imctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage.morphology import disk\n",
    "from skimage.filters import rank\n",
    "from skimage import exposure\n",
    "\n",
    "def normalize(image):\n",
    "        footprint = disk(32)\n",
    "        img = np.log(np.abs(image) - np.min(image) + 1e-7)\n",
    "        p2, p98 = np.percentile(img, (2, 98))\n",
    "        img = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "        img = skimage.util.img_as_ubyte(img)\n",
    "        img_eq = rank.equalize(img, footprint)\n",
    "        img_eq = img_eq.astype(float)/256.0\n",
    "        return img_eq  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = 'data/Nickel/'\n",
    "images = glob(temp_path + '*.tif')\n",
    "\n",
    "i = 0\n",
    "\n",
    "#Time the code below\n",
    "start = time.time()\n",
    "\n",
    "test_im = iio.v2.volread(images[i])\n",
    "test_im_norm = normalize(test_im)\n",
    "\n",
    "\n",
    "#Stop recording time\n",
    "end = time.time()\n",
    "#Plot positions of all nonzero elements in test_im\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(test_im_norm, cmap='binary', origin='lower')\n",
    "plt.title('Raw data')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Time to load image: {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create/load a specific dataset\n",
    "\n",
    "Here, we'll be utilizing the auto-masking process developed in __ to automatically generate labelled/segmented data for artifact identification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples = glob('data/*')\n",
    "mask_folder = 'masks'\n",
    "\n",
    "for path_to_sample in Samples:\n",
    "\n",
    "    path_to_sample = path_to_sample + '/'\n",
    "\n",
    "    if not os.path.isdir(path_to_sample + mask_folder):\n",
    "        #Grabbing imctrl file and parsing its contents. If no imctrl file exists, skip this\n",
    "        try:\n",
    "            imctrl_file = glob(path_to_sample + '*.imctrl')[0]\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            #Assign controls\n",
    "            controls = parse_imctrl(imctrl_file)\n",
    "\n",
    "            #Creating mask, need to specify image size here\n",
    "            #This MASK function was developed from a previous publication by Yanxon et al.\n",
    "            mask = MASK(controls=controls, shape=(2880,2880))\n",
    "\n",
    "            #Creating mask directory\n",
    "            os.mkdir(path_to_sample + mask_folder)\n",
    "            paths = glob(path_to_sample + '*.tif')\n",
    "\n",
    "            #Generate masked image for each sample image and save it in a mask subdirectory for a specific dataset\n",
    "            for path in paths:\n",
    "                image_name = path.split('/')[-1][:-4]\n",
    "                image = iio.v2.volread(path)\n",
    "                result = mask.AutoSpotMask(image, esdmul=7.0)\n",
    "                iio.v2.imwrite(path_to_sample + f'{mask_folder}/{image_name}_mask.tif', result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset/dataloaders for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll grab put all relevant tif files into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset object from data files\n",
    "Samples = [f for f in glob('data/*') if \"Nickel\" in f or \"battery5\" in f]\n",
    "dataset = Dataset(n=len(Samples))\n",
    "\n",
    "#Grab all tif files from directories in Samples\n",
    "dataset.get_data(Samples, label_ext = '.tif')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how much available memory exists \n",
    "#Note: This is not the same as the amount of memory used by the dataset\n",
    "#This is the amount of memory available for the dataset to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "dataset.images[i].size * dataset.images[i].itemsize / 1e6 #size of dataset images in megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our model parameters. This includes quilter params (for stitching pytorch tensors), and model params (for the UNet).\n",
    "\n",
    "The purpose of a quilter here is to cut up a large tensor into smaller overlapping pieces which can then be fed into a GPU card (in case there are memory limitations).\n",
    "\n",
    "Here, we have:<br>\n",
    "*N* = window size (of a patch)<br>\n",
    "*M* = Step size (how much the patch/window is moved)<br>\n",
    "*B* = Border size (Border of window which is multiplied by border weight)<br>\n",
    "\n",
    "E.g. A window size of 256, step size of 128, with a border size of 32 means that\n",
    "we take patches of size (256-64 = 192) x 192. These 192 x 192 blocks overlap with one another, since we only take a step of size 128 when moving to a new patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define quilter hyperparams\n",
    "#Quilter takes a large tensor and cuts it up into smaller pieces in case the whole tensor\n",
    "#does not fit \n",
    "N = 256 #Patch size\n",
    "M = N // 2\n",
    "B = M // 4\n",
    "\n",
    "#Define quilter params\n",
    "quilter_params = {'Y': 2880, 'X': 2880,\n",
    "                  'window': (N, N),\n",
    "                  'step': (M, M),\n",
    "                  'border': (B, B),\n",
    "                  'border_weight': 0}\n",
    "\n",
    "#Define TUNet params (from dlsia)\n",
    "model_params = {'image_shape': (2880, 2880),\n",
    "                'in_channels': 1,\n",
    "                'out_channels': 2,\n",
    "                'base_channels': 8,\n",
    "                'growth_rate': 2,\n",
    "                'depth': 4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define training params as well as define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "epoch = 10\n",
    "batch_size = 50\n",
    "lr_rate = 1e-2\n",
    "\n",
    "model = cmodel(quilter_params, model_params, device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "model.train(dataset, include_data={0: [0],\n",
    "                                   1: [0],\n",
    "                                   2: [0],\n",
    "                                   3: [0],\n",
    "                                   4: [0],\n",
    "                                   5: [0]},\n",
    "            epoch=epoch,\n",
    "            batch_size=batch_size,\n",
    "            lr_rate=lr_rate)\n",
    "\n",
    "model.save('models/test_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airxd-cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
